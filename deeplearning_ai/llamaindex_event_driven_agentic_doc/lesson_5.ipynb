{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7890f6a5",
   "metadata": {},
   "source": [
    "## Lesson 5: Human in the Loop\n",
    "\n",
    "Lesson objective: Get feedback on answers from a human operator\n",
    "\n",
    "In this lab, you'll learn how to make Workflows easy to iterate on answers to the questionnaire by getting feedback on answers from the human operator and re-answering when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3ac69d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage\n",
    "from llama_index.core.base.base_query_engine import BaseQueryEngine\n",
    "from llama_index.core.workflow import (\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    "    Event,\n",
    "    Context,\n",
    ")\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "from llama_parse import LlamaParse, ResultType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a524693",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "llama_cloud_api_key = os.environ[\"LLAMA_CLOUD_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b849447f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2f837e",
   "metadata": {},
   "source": [
    "### Adding a feedback loop\n",
    "\n",
    "The changes you're going to make here are:\n",
    "1. Use the `InputRequiredEvent` and `HumanResponseEvent`, new special events specifically designed to allow you to exit the workflow, and get feedback back into it.\n",
    "2. You used to have a single step which parsed your form and fired off all your questions. Since we now might loop back and ask questions several times, we don't need to parse the form every time, so we'll split up those steps. This kind of refactoring is very common as you create a more complex workflow:\n",
    "   - Your new `generate_questions` step will be triggered either by a `GenerateQuestionsEvent`, triggered by the form parser, or by a `FeedbackEvent`, which is the loop we'll take after getting feedback.\n",
    "3. `fill_in_application` will emit an `InputRequiredEvent`, and in the `external_step` you'll wait for a `HumanResponseEvent`. This will pause the whole workflow waiting for outside input.\n",
    "4. Finally, you'll use the LLM to parse the feedback and decide whether it means you should continue and output the results, or if you need to loop back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec4f2d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new!\n",
    "from llama_index.core.workflow import InputRequiredEvent, HumanResponseEvent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd6ee472",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParseFormEvent(Event):\n",
    "    application_form: str\n",
    "\n",
    "class QueryEvent(Event):\n",
    "    query: str\n",
    "    field: str\n",
    "    \n",
    "class ResponseEvent(Event):\n",
    "    field: str\n",
    "    response: str\n",
    "\n",
    "# new!\n",
    "class FeedbackEvent(Event):\n",
    "    feedback: str\n",
    "\n",
    "class GenerateQuestionsEvent(Event):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6893bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGWorkflow(Workflow):\n",
    "\n",
    "    storage_dir: str = \"./storage\"\n",
    "    llm: OpenAI\n",
    "    query_engine: BaseQueryEngine\n",
    "\n",
    "    @step\n",
    "    async def set_up(self, ctx: Context, ev: StartEvent) -> ParseFormEvent:\n",
    "\n",
    "        if not ev.resume_file:\n",
    "            raise ValueError(\"No resume file provided\")\n",
    "\n",
    "        if not ev.application_form:\n",
    "            raise ValueError(\"No application form provided\")\n",
    "\n",
    "        # define the LLM to work with\n",
    "        self.llm = OpenAI(\n",
    "            model=\"gpt-4.1-nano\",\n",
    "            api_key=openai_api_key,\n",
    "            api_base=os.environ[\"OPENAI_API_BASE\"],\n",
    "            temperature=0.5,\n",
    "        )\n",
    "\n",
    "        # ingest the data and set up the query engine\n",
    "        if os.path.exists(self.storage_dir):\n",
    "            # you've already ingested the resume document\n",
    "            storage_context = StorageContext.from_defaults(persist_dir=self.storage_dir)\n",
    "            index = load_index_from_storage(\n",
    "                storage_context=storage_context,\n",
    "                embed_model=OpenAIEmbedding(\n",
    "                    model_name=\"Cohere-embed-v3-english\",\n",
    "                    api_key=openai_api_key,\n",
    "                    api_base=os.environ[\"OPENAI_API_BASE\"],\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            # parse and load the resume document\n",
    "            documents = LlamaParse(\n",
    "                api_key=llama_cloud_api_key,\n",
    "                # base_url=os.getenv(\"LLAMA_CLOUD_BASE_URL\"),\n",
    "                result_type=ResultType.MD,\n",
    "                user_prompt=\"This is a resume, gather related facts together and format it as bullet points with headers\",\n",
    "            ).load_data(ev.resume_file)\n",
    "            # embed and index the documents\n",
    "            index = VectorStoreIndex.from_documents(\n",
    "                documents=documents,\n",
    "                embed_model=OpenAIEmbedding(\n",
    "                    model_name=\"Cohere-embed-v3-english\",\n",
    "                    api_key=openai_api_key,\n",
    "                    api_base=os.environ[\"OPENAI_API_BASE\"],\n",
    "                ),\n",
    "            )\n",
    "            index.storage_context.persist(persist_dir=self.storage_dir)\n",
    "\n",
    "        # create a query engine\n",
    "        self.query_engine = index.as_query_engine(llm=self.llm, similarity_top_k=5)\n",
    "\n",
    "        # you no longer need a query to be passed in,\n",
    "        # you'll be generating the queries instead\n",
    "        # let's pass the application form to a new step to parse it\n",
    "        return ParseFormEvent(application_form=ev.application_form)\n",
    "\n",
    "    # new - separated the form parsing from the question generation\n",
    "    @step\n",
    "    async def parse_form(\n",
    "        self, ctx: Context, ev: ParseFormEvent\n",
    "    ) -> GenerateQuestionsEvent:\n",
    "        parser = LlamaParse(\n",
    "            api_key=llama_cloud_api_key,\n",
    "            # base_url=os.getenv(\"LLAMA_CLOUD_BASE_URL\"),\n",
    "            result_type=ResultType.MD,\n",
    "            user_prompt=\"This is a job application form.\"\n",
    "            \" Create a list of all the fields that need to be filled in.\"\n",
    "            \" Return a bulleted list of the fields ONLY.\",\n",
    "        )\n",
    "\n",
    "        # get the LLM to convert the parsed form into JSON\n",
    "        result = parser.load_data(ev.application_form)[0]\n",
    "        raw_json = self.llm.complete(\n",
    "            \"This is a parsed form. Convert it into a JSON object containing only the list of fields to be filled in,\"\n",
    "            f\" in the form {{ fields: [...] }}. <form>{result.text}</form>. Return JSON ONLY, no markdown.\"\n",
    "        )\n",
    "        fields = json.loads(raw_json.text)[\"fields\"]\n",
    "\n",
    "        await ctx.store.set(\"fields_to_fill\", fields)\n",
    "\n",
    "        return GenerateQuestionsEvent()\n",
    "\n",
    "    # new - this step can get triggered either by GenerateQuestionsEvent or a FeedbackEvent\n",
    "    @step\n",
    "    async def generate_questions(\n",
    "        self, ctx: Context, ev: GenerateQuestionsEvent | FeedbackEvent\n",
    "    ) -> QueryEvent:\n",
    "\n",
    "        # get the list of fields to fill in\n",
    "        fields = await ctx.store.get(\"fields_to_fill\")\n",
    "\n",
    "        # generate one query for each of the fields, and fire them off\n",
    "        for field in fields:\n",
    "            question = f\"How would you answer this question about the candidate? <field>{field}</field>\"\n",
    "            ctx.send_event(QueryEvent(field=field, query=question))\n",
    "\n",
    "        # store the number of fields so we know how many to wait for later\n",
    "        await ctx.store.set(\"total_fields\", len(fields))\n",
    "\n",
    "    @step\n",
    "    async def ask_question(self, ctx: Context, ev: QueryEvent) -> ResponseEvent:\n",
    "        response = self.query_engine.query(\n",
    "            f\"This is a question about the specific resume we have in our database: {ev.query}\"\n",
    "        )\n",
    "        return ResponseEvent(field=ev.field, response=response.response)\n",
    "\n",
    "    # new - we now emit an InputRequiredEvent\n",
    "    @step\n",
    "    async def fill_in_application(\n",
    "        self, ctx: Context, ev: ResponseEvent\n",
    "    ) -> InputRequiredEvent | None:\n",
    "        # get the total number of fields to wait for\n",
    "        total_fields = await ctx.store.get(\"total_fields\")\n",
    "\n",
    "        responses = ctx.collect_events(ev, [ResponseEvent] * total_fields)\n",
    "        if responses is None:\n",
    "            return  # do nothing if there's nothing to do yet\n",
    "\n",
    "        # we've got all the responses!\n",
    "        responseList = \"\\n\".join(\n",
    "            \"Field: \" + r.field + \"\\n\" + \"Response: \" + r.response for r in responses\n",
    "        )\n",
    "\n",
    "        result = self.llm.complete(\n",
    "            f\"\"\"\n",
    "            You are given a list of fields in an application form and responses to\n",
    "            questions about those fields from a resume. Combine the two into a list of\n",
    "            fields and succinct, factual answers to fill in those fields.\n",
    "\n",
    "            <responses>\n",
    "            {responseList}\n",
    "            </responses>\n",
    "        \"\"\"\n",
    "        )\n",
    "\n",
    "        # new! save the result for later\n",
    "        await ctx.store.set(\"filled_form\", str(result))\n",
    "\n",
    "        # new! Let's get a human in the loop\n",
    "        return InputRequiredEvent(\n",
    "            prefix=\"How does this look? Give me any feedback you have on any of the answers.\",\n",
    "            result=result,\n",
    "        )\n",
    "\n",
    "    # new! Accept the feedback.\n",
    "    @step\n",
    "    async def get_feedback(\n",
    "        self, ctx: Context, ev: HumanResponseEvent\n",
    "    ) -> FeedbackEvent | StopEvent:\n",
    "\n",
    "        result = self.llm.complete(\n",
    "            f\"\"\"\n",
    "            You have received some human feedback on the form-filling task you've done.\n",
    "            Does everything look good, or is there more work to be done?\n",
    "            <feedback>\n",
    "            {ev.response}\n",
    "            </feedback>\n",
    "            If everything is fine, respond with just the word 'OKAY'.\n",
    "            If there's any other feedback, respond with just the word 'FEEDBACK'.\n",
    "        \"\"\"\n",
    "        )\n",
    "\n",
    "        verdict = result.text.strip()\n",
    "\n",
    "        print(f\"LLM says the verdict was {verdict}\")\n",
    "        if verdict == \"OKAY\":\n",
    "            return StopEvent(result=await ctx.store.get(\"filled_form\"))\n",
    "\n",
    "        return FeedbackEvent(feedback=ev.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad647ac0",
   "metadata": {},
   "source": [
    "Okay! Your workflow is now ready to get some feedback, but how do we actually get it? The `InputRequiredEvent` is an event in the event stream, just like the `ProgressEvents` and `TextEvents` you've seen in lesson 2. You can intercept it the same way you did those, and use the `send_event` method on the context to send back a `HumanResponseEvent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fac407c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./storage/docstore.json.\n",
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./storage/index_store.json.\n",
      "Started parsing the file under job_id 20b52d0a-70de-4749-924b-411ae17a7cd2\n",
      "We've filled in your form! Here are the results:\n",
      "\n",
      "- **Position:** Full Stack Developer with experience as Senior Full Stack Developer, specializing in building scalable web applications, microservices, and leading technical teams.  \n",
      "- **First Name:** Sarah  \n",
      "- **Last Name:** Chen  \n",
      "- **Email:** sarah.chen@email.com  \n",
      "- **Phone:** Not provided  \n",
      "- **LinkedIn:** linkedin.com/in/sarahchen  \n",
      "- **Project Portfolio:** Developed a full-stack carbon footprint tracking app with machine learning features recognized in a top environmental app list; created a real-time encrypted chat app serving over 5,000 users; contributed to open source projects; built multiple responsive web applications using various frameworks and technologies.  \n",
      "- **Degree:** Bachelor of Science in Computer Science from the University of California, Berkeley  \n",
      "- **Graduation Date:** 2017  \n",
      "- **Current Job Title:** Senior Full Stack Developer  \n",
      "- **Current Employer:** TechFlow Solutions  \n",
      "- **Technical Skills:** Frontend: React.js, Redux, Next.js, TypeScript, Vue.js, Nuxt.js, HTML5, CSS3, SASS/SCSS, Jest, React Testing Library, WebPack, Babel; Backend: Node.js, Express.js, Python, Django, GraphQL, REST APIs, PostgreSQL, MongoDB.  \n",
      "- **Why are you a good fit for this position:** Extensive full stack development experience with strong expertise in React, Node.js, and cloud architecture. Proven leadership in managing technical teams, implementing scalable solutions, and optimizing performance. Skilled in mentoring junior developers, establishing coding standards, and proficient with CI/CD pipelines and microservices. Certifications in AWS and Google Cloud demonstrate technical proficiency and adaptability. This combination of skills, leadership, and passion for best practices makes the candidate highly suitable.  \n",
      "- **Do you have 5 years of experience in React?:** Yes, over 5 years of experience with React.js, Next.js, and related technologies.\n",
      "LLM says the verdict was FEEDBACK\n",
      "We've filled in your form! Here are the results:\n",
      "\n",
      "- **Position:** The candidate has experience as a Full Stack Developer, including roles such as Senior Full Stack Developer and Full Stack Developer, with expertise in developing scalable web applications, microservices, and leading technical teams.  \n",
      "- **First Name:** Sarah  \n",
      "- **Last Name:** Chen  \n",
      "- **Email:** sarah.chen@email.com  \n",
      "- **Phone:** Not provided in the available resume information.  \n",
      "- **LinkedIn:** linkedin.com/in/sarahchen  \n",
      "- **Project Portfolio:** The candidate’s project portfolio includes a full-stack application for tracking carbon footprints called EcoTrack, which features machine learning for personalized recommendations and was recognized in TechCrunch. They also developed a real-time chat application named ChatFlow, implementing WebSocket, end-to-end encryption, and message persistence, serving over 5,000 monthly users.  \n",
      "- **Degree:** Bachelor of Science in Computer Science with a minor in User Experience Design from the University of California, Berkeley.  \n",
      "- **Graduation Date:** 2017  \n",
      "- **Current Job Title:** Senior Full Stack Developer  \n",
      "- **Current Employer:** TechFlow Solutions in San Francisco, CA  \n",
      "- **Technical Skills:** Strong technical skills in both frontend and backend development. Frontend expertise includes React.js, Redux, Next.js, TypeScript, Vue.js, Nuxt.js, HTML5, CSS3, SASS/SCSS, Jest, React Testing Library, WebPack, and Babel. Backend proficiency with Node.js, Express.js, Python, Django, GraphQL, REST APIs, PostgreSQL, and MongoDB.  \n",
      "- **Describe why you’re a good fit for this position:** Demonstrates extensive experience in full-stack development, leading technical teams, building scalable applications, and optimizing performance. Skilled in modern frontend frameworks like React, Vue.js, and Next.js, with backend expertise in Node.js, Django, and GraphQL. Has successfully implemented CI/CD pipelines, improved code quality, and contributed to accessibility and performance improvements. Mentorship experience and certifications in cloud architecture and database development further showcase leadership and commitment to continuous learning, making them a strong fit for this role.  \n",
      "- **Do you have 5 years of experience in React?** Yes, the candidate has over 5 years of experience with React, including recent roles where they led the rebuilding of products using React and related technologies.\n",
      "LLM says the verdict was OKAY\n",
      "Agent complete! Here's your final result:\n",
      "- **Position:** The candidate has experience as a Full Stack Developer, including roles such as Senior Full Stack Developer and Full Stack Developer, with expertise in developing scalable web applications, microservices, and leading technical teams.  \n",
      "- **First Name:** Sarah  \n",
      "- **Last Name:** Chen  \n",
      "- **Email:** sarah.chen@email.com  \n",
      "- **Phone:** Not provided in the available resume information.  \n",
      "- **LinkedIn:** linkedin.com/in/sarahchen  \n",
      "- **Project Portfolio:** The candidate’s project portfolio includes a full-stack application for tracking carbon footprints called EcoTrack, which features machine learning for personalized recommendations and was recognized in TechCrunch. They also developed a real-time chat application named ChatFlow, implementing WebSocket, end-to-end encryption, and message persistence, serving over 5,000 monthly users.  \n",
      "- **Degree:** Bachelor of Science in Computer Science with a minor in User Experience Design from the University of California, Berkeley.  \n",
      "- **Graduation Date:** 2017  \n",
      "- **Current Job Title:** Senior Full Stack Developer  \n",
      "- **Current Employer:** TechFlow Solutions in San Francisco, CA  \n",
      "- **Technical Skills:** Strong technical skills in both frontend and backend development. Frontend expertise includes React.js, Redux, Next.js, TypeScript, Vue.js, Nuxt.js, HTML5, CSS3, SASS/SCSS, Jest, React Testing Library, WebPack, and Babel. Backend proficiency with Node.js, Express.js, Python, Django, GraphQL, REST APIs, PostgreSQL, and MongoDB.  \n",
      "- **Describe why you’re a good fit for this position:** Demonstrates extensive experience in full-stack development, leading technical teams, building scalable applications, and optimizing performance. Skilled in modern frontend frameworks like React, Vue.js, and Next.js, with backend expertise in Node.js, Django, and GraphQL. Has successfully implemented CI/CD pipelines, improved code quality, and contributed to accessibility and performance improvements. Mentorship experience and certifications in cloud architecture and database development further showcase leadership and commitment to continuous learning, making them a strong fit for this role.  \n",
      "- **Do you have 5 years of experience in React?** Yes, the candidate has over 5 years of experience with React, including recent roles where they led the rebuilding of products using React and related technologies.\n"
     ]
    }
   ],
   "source": [
    "workflow = RAGWorkflow(timeout=600, verbose=False)\n",
    "\n",
    "handler = workflow.run(\n",
    "    resume_file=\"./data/fake_resume.pdf\",\n",
    "    application_form=\"./data/fake_application_form.pdf\",\n",
    ")\n",
    "\n",
    "async for event in handler.stream_events():\n",
    "    if isinstance(event, InputRequiredEvent):\n",
    "        print(\"We've filled in your form! Here are the results:\\n\")\n",
    "        print(event.result)\n",
    "\n",
    "        # now ask for input from the keyboard\n",
    "        response = input(event.prefix)\n",
    "        handler.ctx.send_event(HumanResponseEvent(response=response))\n",
    "\n",
    "response = await handler\n",
    "\n",
    "print(\"Agent complete! Here's your final result:\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d172ef7",
   "metadata": {},
   "source": [
    "### Using the Feedback\n",
    "\n",
    "Okay! Now let's further modify things to actually do something useful with the feedback in `generate_questions` step. This involves checking if there's feedback, and appending it to the questions. In this simple example, we're going to append the feedback to every question in case it's relevant, but a more sophisticated agent might apply it only to the fields where the feedback applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa1f4837",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGWorkflow(Workflow):\n",
    "\n",
    "    storage_dir: str = \"./storage\"\n",
    "    llm: OpenAI\n",
    "    query_engine: BaseQueryEngine\n",
    "\n",
    "    @step\n",
    "    async def set_up(self, ctx: Context, ev: StartEvent) -> ParseFormEvent:\n",
    "\n",
    "        if not ev.resume_file:\n",
    "            raise ValueError(\"No resume file provided\")\n",
    "\n",
    "        if not ev.application_form:\n",
    "            raise ValueError(\"No application form provided\")\n",
    "\n",
    "        # define the LLM to work with\n",
    "        self.llm = OpenAI(\n",
    "            model=\"gpt-4.1-nano\",\n",
    "            api_key=openai_api_key,\n",
    "            api_base=os.environ[\"OPENAI_API_BASE\"],\n",
    "            temperature=0.5,\n",
    "        )\n",
    "\n",
    "        # ingest the data and set up the query engine\n",
    "        if os.path.exists(self.storage_dir):\n",
    "            # you've already ingested the resume document\n",
    "            storage_context = StorageContext.from_defaults(persist_dir=self.storage_dir)\n",
    "            index = load_index_from_storage(\n",
    "                storage_context=storage_context,\n",
    "                embed_model=OpenAIEmbedding(\n",
    "                    model_name=\"Cohere-embed-v3-english\",\n",
    "                    api_key=openai_api_key,\n",
    "                    api_base=os.environ[\"OPENAI_API_BASE\"],\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            # parse and load the resume document\n",
    "            documents = LlamaParse(\n",
    "                api_key=llama_cloud_api_key,\n",
    "                # base_url=os.getenv(\"LLAMA_CLOUD_BASE_URL\"),\n",
    "                result_type=ResultType.MD,\n",
    "                user_prompt=\"This is a resume, gather related facts together and format it as bullet points with headers\",\n",
    "            ).load_data(ev.resume_file)\n",
    "            # embed and index the documents\n",
    "            index = VectorStoreIndex.from_documents(\n",
    "                documents=documents,\n",
    "                embed_model=OpenAIEmbedding(\n",
    "                    model_name=\"Cohere-embed-v3-english\",\n",
    "                    api_key=openai_api_key,\n",
    "                    api_base=os.environ[\"OPENAI_API_BASE\"],\n",
    "                ),\n",
    "            )\n",
    "            index.storage_context.persist(persist_dir=self.storage_dir)\n",
    "\n",
    "        # create a query engine\n",
    "        self.query_engine = index.as_query_engine(llm=self.llm, similarity_top_k=5)\n",
    "\n",
    "        # let's pass the application form to a new step to parse it\n",
    "        return ParseFormEvent(application_form=ev.application_form)\n",
    "\n",
    "    # form parsing\n",
    "    @step\n",
    "    async def parse_form(\n",
    "        self, ctx: Context, ev: ParseFormEvent\n",
    "    ) -> GenerateQuestionsEvent:\n",
    "        parser = LlamaParse(\n",
    "            api_key=llama_cloud_api_key,\n",
    "            # base_url=os.getenv(\"LLAMA_CLOUD_BASE_URL\"),\n",
    "            result_type=ResultType.MD,\n",
    "            user_prompt=\"This is a job application form.\"\n",
    "            \" Create a list of all the fields that need to be filled in.\"\n",
    "            \" Return a bulleted list of the fields ONLY.\",\n",
    "        )\n",
    "\n",
    "        # get the LLM to convert the parsed form into JSON\n",
    "        result = parser.load_data(ev.application_form)[0]\n",
    "        raw_json = self.llm.complete(\n",
    "            \"This is a parsed form. Convert it into a JSON object containing only the list of fields to be filled in,\"\n",
    "            f\" in the form {{ fields: [...] }}. <form>{result.text}</form>. Return JSON ONLY, no markdown.\"\n",
    "        )\n",
    "        fields = json.loads(raw_json.text)[\"fields\"]\n",
    "\n",
    "        await ctx.store.set(\"fields_to_fill\", fields)\n",
    "\n",
    "        return GenerateQuestionsEvent()\n",
    "\n",
    "    # generate questions\n",
    "    @step\n",
    "    async def generate_questions(\n",
    "        self, ctx: Context, ev: GenerateQuestionsEvent | FeedbackEvent\n",
    "    ) -> QueryEvent:\n",
    "\n",
    "        # get the list of fields to fill in\n",
    "        fields = await ctx.store.get(\"fields_to_fill\")\n",
    "\n",
    "        # generate one query for each of the fields, and fire them off\n",
    "        for field in fields:\n",
    "            question = f\"How would you answer this question about the candidate? <field>{field}</field>\"\n",
    "\n",
    "            # new! Is there feedback? If so, add it to the query:\n",
    "            if hasattr(ev, \"feedback\"):\n",
    "                question += f\"\"\"\n",
    "                    \\nWe previously got feedback about how we answered the questions.\n",
    "                    It might not be relevant to this particular field, but here it is:\n",
    "                    <feedback>{ev.feedback}</feedback>\n",
    "                \"\"\"\n",
    "\n",
    "            ctx.send_event(QueryEvent(field=field, query=question))\n",
    "\n",
    "        # store the number of fields so we know how many to wait for later\n",
    "        await ctx.store.set(\"total_fields\", len(fields))\n",
    "\n",
    "    @step\n",
    "    async def ask_question(self, ctx: Context, ev: QueryEvent) -> ResponseEvent:\n",
    "        response = self.query_engine.query(\n",
    "            f\"This is a question about the specific resume we have in our database: {ev.query}\"\n",
    "        )\n",
    "        return ResponseEvent(field=ev.field, response=response.response)\n",
    "\n",
    "    # Get feedback from the human\n",
    "    @step\n",
    "    async def fill_in_application(\n",
    "        self, ctx: Context, ev: ResponseEvent\n",
    "    ) -> InputRequiredEvent | None:\n",
    "        # get the total number of fields to wait for\n",
    "        total_fields = await ctx.store.get(\"total_fields\")\n",
    "\n",
    "        responses = ctx.collect_events(ev, [ResponseEvent] * total_fields)\n",
    "        if responses is None:\n",
    "            return  # do nothing if there's nothing to do yet\n",
    "\n",
    "        # we've got all the responses!\n",
    "        responseList = \"\\n\".join(\n",
    "            \"Field: \" + r.field + \"\\n\" + \"Response: \" + r.response for r in responses\n",
    "        )\n",
    "\n",
    "        result = self.llm.complete(\n",
    "            f\"\"\"\n",
    "            You are given a list of fields in an application form and responses to\n",
    "            questions about those fields from a resume. Combine the two into a list of\n",
    "            fields and succinct, factual answers to fill in those fields.\n",
    "\n",
    "            <responses>\n",
    "            {responseList}\n",
    "            </responses>\n",
    "        \"\"\"\n",
    "        )\n",
    "\n",
    "        # save the result for later\n",
    "        await ctx.store.set(\"filled_form\", str(result))\n",
    "\n",
    "        # Fire off the feedback request\n",
    "        return InputRequiredEvent(\n",
    "            prefix=\"How does this look? Give me any feedback you have on any of the answers.\",\n",
    "            result=result,\n",
    "        )\n",
    "\n",
    "    # Accept the feedback when a HumanResponseEvent fires\n",
    "    @step\n",
    "    async def get_feedback(\n",
    "        self, ctx: Context, ev: HumanResponseEvent\n",
    "    ) -> FeedbackEvent | StopEvent:\n",
    "\n",
    "        result = self.llm.complete(\n",
    "            f\"\"\"\n",
    "            You have received some human feedback on the form-filling task you've done.\n",
    "            Does everything look good, or is there more work to be done?\n",
    "            <feedback>\n",
    "            {ev.response}\n",
    "            </feedback>\n",
    "            If everything is fine, respond with just the word 'OKAY'.\n",
    "            If there's any other feedback, respond with just the word 'FEEDBACK'.\n",
    "        \"\"\"\n",
    "        )\n",
    "\n",
    "        verdict = result.text.strip()\n",
    "        print(f\"LLM says the verdict was {verdict}\")\n",
    "\n",
    "        if verdict == \"OKAY\":\n",
    "            return StopEvent(result=await ctx.store.get(\"filled_form\"))\n",
    "\n",
    "        return FeedbackEvent(feedback=ev.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1eaaa26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./storage/docstore.json.\n",
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./storage/index_store.json.\n",
      "Started parsing the file under job_id 5267a51c-4462-45bf-8d77-a47ed24de7ca\n",
      "We've filled in your form! Here are the results:\n",
      "\n",
      "- **Position:** The candidate has experience as a Full Stack Developer, including roles such as Senior Full Stack Developer and Full Stack Developer at various companies.  \n",
      "- **First Name:** Sarah  \n",
      "- **Last Name:** Chen  \n",
      "- **Email:** sarah.chen@email.com  \n",
      "- **Phone:** Not provided in the available information  \n",
      "- **LinkedIn:** linkedin.com/in/sarahchen  \n",
      "- **Project Portfolio:** Includes EcoTrack, a full-stack app for tracking carbon footprints with machine learning features, featured in TechCrunch's \"Top 10 Environmental Impact Apps of 2023\"; and ChatFlow, a real-time chat app using WebSocket with end-to-end encryption, serving over 5,000 monthly active users.  \n",
      "- **Degree:** Bachelor of Science in Computer Science from the University of California, Berkeley  \n",
      "- **Graduation Date:** 2017  \n",
      "- **Current Job Title:** Senior Full Stack Developer  \n",
      "- **Current Employer:** TechFlow Solutions  \n",
      "- **Technical Skills:** Frontend: React.js, Redux, Next.js, TypeScript, Vue.js, Nuxt.js, HTML5, CSS3, SASS/SCSS, Jest, React Testing Library, WebPack, Babel; Backend: Node.js, Express.js, Python, Django, GraphQL, REST APIs, PostgreSQL, MongoDB.  \n",
      "- **Describe why you’re a good fit for this position:** Over six years of full-stack development experience, designing scalable applications, leading technical teams, and implementing CI/CD pipelines. Proficient in React, Node.js, Vue.js, and cloud services. Skilled in optimizing performance, enhancing code quality, and mentoring junior developers. Strong background in accessibility, testing, and modern development practices.  \n",
      "- **Do you have 5 years of experience in React?** Yes, with over 5 years of experience including React.js, Redux, Next.js, TypeScript, and building full-stack applications with React.\n",
      "LLM says the verdict was FEEDBACK\n",
      "We've filled in your form! Here are the results:\n",
      "\n",
      "- Position: Full Stack Developer, with experience as Junior Web Developer and Senior Full Stack Developer  \n",
      "- First Name: Sarah  \n",
      "- Last Name: Chen  \n",
      "- Email: sarah.chen@email.com  \n",
      "- Phone: Not provided  \n",
      "- Linkedin: linkedin.com/in/sarahchen  \n",
      "- Project Portfolio: GitHub profile at github.com/sarahcodes and personal website at sarahchen.dev  \n",
      "- Degree: Bachelor of Science in Computer Science from the University of California, Berkeley  \n",
      "- Graduation Date: 2017  \n",
      "- Current Job Title: Senior Full Stack Developer  \n",
      "- Current Employer: TechFlow Solutions  \n",
      "- Technical Skills: Frontend - React.js, Redux, Next.js, TypeScript, Vue.js, Nuxt.js, HTML5, CSS3, SASS/SCSS, Jest, React Testing Library, WebPack, Babel; Backend - Node.js, Express.js, Python, Django, GraphQL, REST APIs, PostgreSQL, MongoDB  \n",
      "- Why I’m a good fit: Extensive full stack development experience, expertise in modern frameworks, leadership in technical teams, performance optimization, cloud architecture, CI/CD, accessibility standards, and a strong project portfolio including EcoTrack and ChatFlow.  \n",
      "- React Experience: Over 5 years of experience working with React.js and related frameworks, including building responsive applications and leading React-based projects.\n",
      "LLM says the verdict was OKAY\n",
      "Agent complete! Here's your final result:\n",
      "- Position: Full Stack Developer, with experience as Junior Web Developer and Senior Full Stack Developer  \n",
      "- First Name: Sarah  \n",
      "- Last Name: Chen  \n",
      "- Email: sarah.chen@email.com  \n",
      "- Phone: Not provided  \n",
      "- Linkedin: linkedin.com/in/sarahchen  \n",
      "- Project Portfolio: GitHub profile at github.com/sarahcodes and personal website at sarahchen.dev  \n",
      "- Degree: Bachelor of Science in Computer Science from the University of California, Berkeley  \n",
      "- Graduation Date: 2017  \n",
      "- Current Job Title: Senior Full Stack Developer  \n",
      "- Current Employer: TechFlow Solutions  \n",
      "- Technical Skills: Frontend - React.js, Redux, Next.js, TypeScript, Vue.js, Nuxt.js, HTML5, CSS3, SASS/SCSS, Jest, React Testing Library, WebPack, Babel; Backend - Node.js, Express.js, Python, Django, GraphQL, REST APIs, PostgreSQL, MongoDB  \n",
      "- Why I’m a good fit: Extensive full stack development experience, expertise in modern frameworks, leadership in technical teams, performance optimization, cloud architecture, CI/CD, accessibility standards, and a strong project portfolio including EcoTrack and ChatFlow.  \n",
      "- React Experience: Over 5 years of experience working with React.js and related frameworks, including building responsive applications and leading React-based projects.\n"
     ]
    }
   ],
   "source": [
    "workflow = RAGWorkflow(timeout=600, verbose=False)\n",
    "handler = workflow.run(\n",
    "    resume_file=\"./data/fake_resume.pdf\",\n",
    "    application_form=\"./data/fake_application_form.pdf\",\n",
    ")\n",
    "\n",
    "async for event in handler.stream_events():\n",
    "    if isinstance(event, InputRequiredEvent):\n",
    "        print(\"We've filled in your form! Here are the results:\\n\")\n",
    "        print(event.result)\n",
    "\n",
    "        # now ask for input from the keyboard\n",
    "        response = input(event.prefix)\n",
    "        handler.ctx.send_event(HumanResponseEvent(response=response))\n",
    "\n",
    "response = await handler\n",
    "\n",
    "print(\"Agent complete! Here's your final result:\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e36b92",
   "metadata": {},
   "source": [
    "### Workflow Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7c69048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workflows/feedback_workflow.html\n"
     ]
    }
   ],
   "source": [
    "WORKFLOW_FILE = \"workflows/feedback_workflow.html\"\n",
    "draw_all_possible_flows(workflow, filename=WORKFLOW_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a436341",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
