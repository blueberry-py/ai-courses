{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd2a0958",
   "metadata": {},
   "source": [
    "## Lesson 6: Use your voice\n",
    "\n",
    "**Lesson objective**: Get voice feedback \n",
    "\n",
    "So far we've set up a moderately complex workflow with a human feedback loop. Let's run it through the visualizer to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c447541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage\n",
    "from llama_index.core.base.base_query_engine import BaseQueryEngine\n",
    "from llama_index.core.workflow import (\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    "    Event,\n",
    "    Context,\n",
    "    InputRequiredEvent,\n",
    "    HumanResponseEvent,\n",
    ")\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "from llama_parse import LlamaParse, ResultType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63677cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "llama_cloud_api_key = os.environ[\"LLAMA_CLOUD_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef4187ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "355b081c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParseFormEvent(Event):\n",
    "    application_form: str\n",
    "\n",
    "class QueryEvent(Event):\n",
    "    query: str\n",
    "    field: str\n",
    "    \n",
    "class ResponseEvent(Event):\n",
    "    field: str\n",
    "    response: str\n",
    "\n",
    "class FeedbackEvent(Event):\n",
    "    feedback: str\n",
    "\n",
    "class GenerateQuestionsEvent(Event):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795fc5ff",
   "metadata": {},
   "source": [
    "This is exactly the same workflow from lesson 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f0e4506",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGWorkflow(Workflow):\n",
    "\n",
    "    storage_dir: str = \"./storage\"\n",
    "    llm: OpenAI\n",
    "    query_engine: BaseQueryEngine\n",
    "\n",
    "    @step\n",
    "    async def set_up(self, ctx: Context, ev: StartEvent) -> ParseFormEvent:\n",
    "\n",
    "        if not ev.resume_file:\n",
    "            raise ValueError(\"No resume file provided\")\n",
    "\n",
    "        if not ev.application_form:\n",
    "            raise ValueError(\"No application form provided\")\n",
    "\n",
    "        # define the LLM to work with\n",
    "        self.llm = OpenAI(\n",
    "            model=\"gpt-4.1-nano\",\n",
    "            api_key=openai_api_key,\n",
    "            api_base=os.environ[\"OPENAI_API_BASE\"],\n",
    "            temperature=0.5,\n",
    "        )\n",
    "\n",
    "        # ingest the data and set up the query engine\n",
    "        if os.path.exists(self.storage_dir):\n",
    "            # you've already ingested the resume document\n",
    "            storage_context = StorageContext.from_defaults(persist_dir=self.storage_dir)\n",
    "            index = load_index_from_storage(\n",
    "                storage_context=storage_context,\n",
    "                embed_model=OpenAIEmbedding(\n",
    "                    model_name=\"Cohere-embed-v3-english\",\n",
    "                    api_key=openai_api_key,\n",
    "                    api_base=os.environ[\"OPENAI_API_BASE\"],\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            # parse and load the resume document\n",
    "            documents = LlamaParse(\n",
    "                api_key=llama_cloud_api_key,\n",
    "                # base_url=os.getenv(\"LLAMA_CLOUD_BASE_URL\"),\n",
    "                result_type=ResultType.MD,\n",
    "                user_prompt=\"This is a resume, gather related facts together and format it as bullet points with headers\",\n",
    "            ).load_data(ev.resume_file)\n",
    "            # embed and index the documents\n",
    "            index = VectorStoreIndex.from_documents(\n",
    "                documents=documents,\n",
    "                embed_model=OpenAIEmbedding(\n",
    "                    model_name=\"Cohere-embed-v3-english\",\n",
    "                    api_key=openai_api_key,\n",
    "                    api_base=os.environ[\"OPENAI_API_BASE\"],\n",
    "                ),\n",
    "            )\n",
    "            index.storage_context.persist(persist_dir=self.storage_dir)\n",
    "\n",
    "        # create a query engine\n",
    "        self.query_engine = index.as_query_engine(llm=self.llm, similarity_top_k=5)\n",
    "\n",
    "        # let's pass the application form to a new step to parse it\n",
    "        return ParseFormEvent(application_form=ev.application_form)\n",
    "\n",
    "    # form parsing\n",
    "    @step\n",
    "    async def parse_form(\n",
    "        self, ctx: Context, ev: ParseFormEvent\n",
    "    ) -> GenerateQuestionsEvent:\n",
    "        parser = LlamaParse(\n",
    "            api_key=llama_cloud_api_key,\n",
    "            # base_url=os.getenv(\"LLAMA_CLOUD_BASE_URL\"),\n",
    "            result_type=ResultType.MD,\n",
    "            user_prompt=\"This is a job application form.\"\n",
    "            \" Create a list of all the fields that need to be filled in.\"\n",
    "            \" Return a bulleted list of the fields ONLY.\",\n",
    "        )\n",
    "\n",
    "        # get the LLM to convert the parsed form into JSON\n",
    "        result = parser.load_data(ev.application_form)[0]\n",
    "        raw_json = self.llm.complete(\n",
    "            \"This is a parsed form. Convert it into a JSON object containing only the list of fields to be filled in,\"\n",
    "            f\" in the form {{ fields: [...] }}. <form>{result.text}</form>. Return JSON ONLY, no markdown.\"\n",
    "        )\n",
    "        fields = json.loads(raw_json.text)[\"fields\"]\n",
    "\n",
    "        await ctx.store.set(\"fields_to_fill\", fields)\n",
    "\n",
    "        return GenerateQuestionsEvent()\n",
    "\n",
    "    # generate questions\n",
    "    @step\n",
    "    async def generate_questions(\n",
    "        self, ctx: Context, ev: GenerateQuestionsEvent | FeedbackEvent\n",
    "    ) -> QueryEvent:\n",
    "\n",
    "        # get the list of fields to fill in\n",
    "        fields = await ctx.store.get(\"fields_to_fill\")\n",
    "\n",
    "        # generate one query for each of the fields, and fire them off\n",
    "        for field in fields:\n",
    "            question = f\"How would you answer this question about the candidate? <field>{field}</field>\"\n",
    "\n",
    "            # new! Is there feedback? If so, add it to the query:\n",
    "            if hasattr(ev, \"feedback\"):\n",
    "                question += f\"\"\"\n",
    "                    \\nWe previously got feedback about how we answered the questions.\n",
    "                    It might not be relevant to this particular field, but here it is:\n",
    "                    <feedback>{ev.feedback}</feedback>\n",
    "                \"\"\"\n",
    "\n",
    "            ctx.send_event(QueryEvent(field=field, query=question))\n",
    "\n",
    "        # store the number of fields so we know how many to wait for later\n",
    "        await ctx.store.set(\"total_fields\", len(fields))\n",
    "\n",
    "    @step\n",
    "    async def ask_question(self, ctx: Context, ev: QueryEvent) -> ResponseEvent:\n",
    "        response = self.query_engine.query(\n",
    "            f\"This is a question about the specific resume we have in our database: {ev.query}\"\n",
    "        )\n",
    "        return ResponseEvent(field=ev.field, response=response.response)\n",
    "\n",
    "    # Get feedback from the human\n",
    "    @step\n",
    "    async def fill_in_application(\n",
    "        self, ctx: Context, ev: ResponseEvent\n",
    "    ) -> InputRequiredEvent | None:\n",
    "        # get the total number of fields to wait for\n",
    "        total_fields = await ctx.store.get(\"total_fields\")\n",
    "\n",
    "        responses = ctx.collect_events(ev, [ResponseEvent] * total_fields)\n",
    "        if responses is None:\n",
    "            return  # do nothing if there's nothing to do yet\n",
    "\n",
    "        # we've got all the responses!\n",
    "        responseList = \"\\n\".join(\n",
    "            \"Field: \" + r.field + \"\\n\" + \"Response: \" + r.response for r in responses\n",
    "        )\n",
    "\n",
    "        result = self.llm.complete(\n",
    "            f\"\"\"\n",
    "            You are given a list of fields in an application form and responses to\n",
    "            questions about those fields from a resume. Combine the two into a list of\n",
    "            fields and succinct, factual answers to fill in those fields.\n",
    "\n",
    "            <responses>\n",
    "            {responseList}\n",
    "            </responses>\n",
    "        \"\"\"\n",
    "        )\n",
    "\n",
    "        # save the result for later\n",
    "        await ctx.store.set(\"filled_form\", str(result))\n",
    "\n",
    "        # Fire off the feedback request\n",
    "        return InputRequiredEvent(\n",
    "            prefix=\"How does this look? Give me any feedback you have on any of the answers.\",\n",
    "            result=result,\n",
    "        )\n",
    "\n",
    "    # Accept the feedback when a HumanResponseEvent fires\n",
    "    @step\n",
    "    async def get_feedback(\n",
    "        self, ctx: Context, ev: HumanResponseEvent\n",
    "    ) -> FeedbackEvent | StopEvent:\n",
    "\n",
    "        result = self.llm.complete(\n",
    "            f\"\"\"\n",
    "            You have received some human feedback on the form-filling task you've done.\n",
    "            Does everything look good, or is there more work to be done?\n",
    "            <feedback>\n",
    "            {ev.response}\n",
    "            </feedback>\n",
    "            If everything is fine, respond with just the word 'OKAY'.\n",
    "            If there's any other feedback, respond with just the word 'FEEDBACK'.\n",
    "        \"\"\"\n",
    "        )\n",
    "\n",
    "        verdict = result.text.strip()\n",
    "        print(f\"LLM says the verdict was {verdict}\")\n",
    "\n",
    "        if verdict == \"OKAY\":\n",
    "            return StopEvent(result=await ctx.store.get(\"filled_form\"))\n",
    "\n",
    "        return FeedbackEvent(feedback=ev.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06afe63",
   "metadata": {},
   "source": [
    "### Getting voice feedback\n",
    "\n",
    "Now, just for fun, you'll do one more thing: change the feedback from text feedback to actual words spoken out loud. To do this we'll use a different model from OpenAI called Whisper. LlamaIndex has a built-in way to transcribe audio files into text using Whisper.\n",
    "\n",
    "Here's a function that takes a file and uses Whisper to return just the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cb87ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from queue import Queue\n",
    "\n",
    "import gradio as gr\n",
    "from llama_index.readers.whisper import WhisperReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2b38ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_speech(filepath: str) -> str:\n",
    "    if filepath is None:\n",
    "        gr.Warning(\"No audio found, please retry.\")\n",
    "\n",
    "    # audio_file = open(filepath, \"rb\")\n",
    "\n",
    "    reader = WhisperReader(\n",
    "        model=\"whisper-1\",\n",
    "        api_key=openai_api_key,\n",
    "    )\n",
    "\n",
    "    documents = reader.load_data(filepath)\n",
    "\n",
    "    return documents[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cb7f587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_transcription(output: str) -> str:\n",
    "    global transcription_value\n",
    "    transcription_value = output\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "832fd5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mic_transcribe = gr.Interface(\n",
    "    fn=lambda x: store_transcription(transcribe_speech(x)),\n",
    "    inputs=gr.Audio(sources=\"microphone\", type=\"filepath\"),\n",
    "    outputs=gr.Textbox(label=\"Transcription\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663b73c9",
   "metadata": {},
   "source": [
    "In Gradio, you further define a visual interface containing this microphone input and output, and then launch it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "823e4fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:8000\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:8000/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_interface = gr.Blocks()\n",
    "\n",
    "with test_interface:\n",
    "    gr.TabbedInterface(\n",
    "        [mic_transcribe],\n",
    "        [\"Transcribe Microphone\"]\n",
    "    )\n",
    "\n",
    "test_interface.launch(\n",
    "    share=False, \n",
    "    server_port=8000, \n",
    "    prevent_thread_lock=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cd727c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transcription_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b82acdde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 8000\n"
     ]
    }
   ],
   "source": [
    "test_interface.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52de6f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New! Transcription handler.\n",
    "class TranscriptionHandler:\n",
    "\n",
    "    # we create a queue to hold transcription values\n",
    "    def __init__(self):\n",
    "        self.transcription_queue = Queue()\n",
    "        self.interface = None\n",
    "\n",
    "    # every time we record something we put it in the queue\n",
    "    def store_transcription(self, output):\n",
    "        self.transcription_queue.put(output)\n",
    "        return output\n",
    "\n",
    "    # This is the same interface and transcription logic as before\n",
    "    # except it stores the result in a queue instead of a global\n",
    "    def create_interface(self):\n",
    "        mic_transcribe = gr.Interface(\n",
    "            fn=lambda x: self.store_transcription(transcribe_speech(x)),\n",
    "            inputs=gr.Audio(sources=\"microphone\", type=\"filepath\"),\n",
    "            outputs=gr.Textbox(label=\"Transcription\"),\n",
    "        )\n",
    "        self.interface = gr.Blocks()\n",
    "        with self.interface:\n",
    "            gr.TabbedInterface([mic_transcribe], [\"Transcribe Microphone\"])\n",
    "        return self.interface\n",
    "\n",
    "    # we launch the transcription interface\n",
    "    async def get_transcription(self):\n",
    "        self.interface = self.create_interface()\n",
    "        self.interface.launch(share=False, server_port=8000, prevent_thread_lock=True)\n",
    "\n",
    "        # we poll every 1.5 seconds waiting for something to end up in the queue\n",
    "        while True:\n",
    "            if not self.transcription_queue.empty():\n",
    "                result = self.transcription_queue.get()\n",
    "\n",
    "                if self.interface is not None:\n",
    "                    self.interface.close()\n",
    "\n",
    "                return result\n",
    "\n",
    "            await asyncio.sleep(1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a234f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = RAGWorkflow(timeout=600, verbose=False)\n",
    "\n",
    "handler = workflow.run(\n",
    "    resume_file=\"./data/fake_resume.pdf\",\n",
    "    application_form=\"./data/fake_application_form.pdf\",\n",
    ")\n",
    "\n",
    "async for event in handler.stream_events():\n",
    "    if isinstance(event, InputRequiredEvent):\n",
    "        # Get transcription\n",
    "        transcription_handler = TranscriptionHandler()\n",
    "        response = await transcription_handler.get_transcription()\n",
    "\n",
    "        handler.ctx.send_event(HumanResponseEvent(response=response))\n",
    "\n",
    "response = await handler\n",
    "\n",
    "print(\"Agent complete! Here's your final result:\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8cb004",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "To learn more about agentic document workflows, you check this [article](https://www.llamaindex.ai/blog/introducing-agentic-document-workflows) and theses [example implementations](https://github.com/run-llama/llamacloud-demo/tree/main/examples/document_workflows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac364530",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
