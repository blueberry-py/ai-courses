{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0ff2387",
   "metadata": {},
   "source": [
    "## Lesson 4: Form Parsing\n",
    "\n",
    "**Lesson objective**: Incorporate form parsing to the workflow\n",
    "\n",
    "In your previous lesson, you used LlamaParse to parse a resume, and included parsing instructions. You'll do that again this time, but the instructions are going to be more advanced -- you're going to get it to read an application form and convert it into a list of fields that need to be filled in, and return that as a JSON object. You will then incorporate these steps in the workflow you started building in the previous lesson.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8762939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage\n",
    "from llama_index.core.base.base_query_engine import BaseQueryEngine\n",
    "from llama_index.core.workflow import (\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    "    Event,\n",
    "    Context,\n",
    ")\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "from llama_parse import LlamaParse, ResultType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cccb622",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "llama_cloud_api_key = os.environ[\"LLAMA_CLOUD_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ade467cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18758029",
   "metadata": {},
   "source": [
    "### Parsing an Application Form with LlamaParse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce177be",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = LlamaParse(\n",
    "    api_key=llama_cloud_api_key,\n",
    "    # base_url=os.getenv(\"LLAMA_CLOUD_BASE_URL\"),\n",
    "    result_type=ResultType.MD,\n",
    "    user_prompt=\"This is a job application form.\"\n",
    "    \" Create a list of all the fields that need to be filled in.\"\n",
    "    \" Return a bulleted list of the fields ONLY.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdee4b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: formatting_instruction is deprecated and may be remove in a future release. Use system_prompt, system_prompt_append or user_prompt instead.\n",
      "Started parsing the file under job_id 5c6e4cc6-2c32-465a-99fa-39a653e359bc\n",
      "- First Name\n",
      "- Last Name\n",
      "- Email\n",
      "- Phone\n",
      "- LinkedIn\n",
      "- Project Portfolio\n",
      "- Degree\n",
      "- Graduation Date\n",
      "- Current Job Title\n",
      "- Current Employer\n",
      "- Technical Skills\n",
      "- Describe why you’re a good fit for this position\n",
      "- Do you have 5 years of experience in React?\n"
     ]
    }
   ],
   "source": [
    "result = parser.load_data(file_path=\"./data/fake_application_form.pdf\")[0]\n",
    "\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd727337",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    api_key=openai_api_key,\n",
    "    api_base=os.environ[\"OPENAI_API_BASE\"],\n",
    "    temperature=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35cfbab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"fields\": [\"First Name\", \"Last Name\", \"Email\", \"Phone\", \"LinkedIn\", \"Project Portfolio\", \"Degree\", \"Graduation Date\", \"Current Job Title\", \"Current Employer\", \"Technical Skills\", \"Describe why you’re a good fit for this position\", \"Do you have 5 years of experience in React?\"]}\n"
     ]
    }
   ],
   "source": [
    "raw_json = llm.complete(\n",
    "    prompt=f\"\"\"\n",
    "    This is a parsed form.\n",
    "    Convert it into a JSON object containing only the list \n",
    "    of fields to be filled in, in the form {{ fields: [...] }}. \n",
    "    <form>{result.text}</form>. \n",
    "    Return JSON ONLY, no markdown.\"\"\"\n",
    ")\n",
    "\n",
    "print(raw_json.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2fd199b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Name\n",
      "Last Name\n",
      "Email\n",
      "Phone\n",
      "LinkedIn\n",
      "Project Portfolio\n",
      "Degree\n",
      "Graduation Date\n",
      "Current Job Title\n",
      "Current Employer\n",
      "Technical Skills\n",
      "Describe why you’re a good fit for this position\n",
      "Do you have 5 years of experience in React?\n"
     ]
    }
   ],
   "source": [
    "fields = json.loads(raw_json.text)[\"fields\"]\n",
    "\n",
    "for field in fields:\n",
    "    print(field)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92358c3d",
   "metadata": {},
   "source": [
    "### Adding a Form Parser to the Workflow (first update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93275535",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParseFormEvent(Event):\n",
    "    application_form: str\n",
    "\n",
    "\n",
    "class QueryEvent(Event):\n",
    "    query: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "312deb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGWorkflow(Workflow):\n",
    "\n",
    "    storage_dir = \"./storage\"\n",
    "    llm: OpenAI\n",
    "    query_engine: BaseQueryEngine\n",
    "\n",
    "    @step\n",
    "    async def set_up(self, ctx: Context, ev: StartEvent) -> ParseFormEvent:\n",
    "\n",
    "        if not ev.resume_file:\n",
    "            raise ValueError(\"No resume file provided\")\n",
    "\n",
    "        if not ev.application_form:\n",
    "            raise ValueError(\"No application form provided\")\n",
    "\n",
    "        # define the LLM to work with\n",
    "        self.llm = OpenAI(\n",
    "            model=\"gpt-4.1-nano\",\n",
    "            api_key=openai_api_key,\n",
    "            api_base=os.environ[\"OPENAI_API_BASE\"],\n",
    "            temperature=0.5,\n",
    "        )\n",
    "\n",
    "        # ingest the data and set up the query engine\n",
    "        if os.path.exists(self.storage_dir):\n",
    "            # you've already ingested the resume document\n",
    "            storage_context = StorageContext.from_defaults(persist_dir=self.storage_dir)\n",
    "            index = load_index_from_storage(\n",
    "                storage_context=storage_context,\n",
    "                embed_model=OpenAIEmbedding(\n",
    "                    model_name=\"Cohere-embed-v3-english\",\n",
    "                    api_key=openai_api_key,\n",
    "                    api_base=os.environ[\"OPENAI_API_BASE\"],\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            # parse and load the resume document\n",
    "            documents = LlamaParse(\n",
    "                api_key=llama_cloud_api_key,\n",
    "                # base_url=os.getenv(\"LLAMA_CLOUD_BASE_URL\"),\n",
    "                result_type=ResultType.MD,\n",
    "                content_guideline_instruction=(\n",
    "                    \"This is a resume, gather related facts together and format it as bullet points with headers\"\n",
    "                ),\n",
    "            ).load_data(ev.resume_file)\n",
    "\n",
    "            # embed and index the documents\n",
    "            index = VectorStoreIndex.from_documents(\n",
    "                documents,\n",
    "                embed_model=OpenAIEmbedding(\n",
    "                    model_name=\"Cohere-embed-v3-english\",\n",
    "                    api_key=openai_api_key,\n",
    "                    api_base=os.environ[\"OPENAI_API_BASE\"],\n",
    "                ),\n",
    "            )\n",
    "            index.storage_context.persist(persist_dir=self.storage_dir)\n",
    "\n",
    "        # create a query engine\n",
    "        self.query_engine = index.as_query_engine(llm=self.llm, similarity_top_k=5)\n",
    "\n",
    "        # you no longer need a query to be passed in,\n",
    "        # you'll be generating the queries instead\n",
    "        # let's pass the application form to a new step to parse it\n",
    "        return ParseFormEvent(application_form=ev.application_form)\n",
    "\n",
    "    @step\n",
    "    async def parse_form(self, ctx: Context, ev: ParseFormEvent) -> QueryEvent:\n",
    "        parser = LlamaParse(\n",
    "            api_key=llama_cloud_api_key,\n",
    "            # base_url=os.getenv(\"LLAMA_CLOUD_BASE_URL\"),\n",
    "            result_type=ResultType.MD,\n",
    "            user_prompt=\"This is a job application form.\"\n",
    "            \" Create a list of all the fields that need to be filled in.\"\n",
    "            \" Return a bulleted list of the fields ONLY.\",\n",
    "        )\n",
    "\n",
    "        # get the LLM to convert the parsed form into JSON\n",
    "        result = parser.load_data(ev.application_form)[0]\n",
    "        raw_json = self.llm.complete(\n",
    "            prompt=f\"\"\"\n",
    "            This is a parsed form. \n",
    "            Convert it into a JSON object containing only the list \n",
    "            of fields to be filled in, in the form {{ fields: [...] }}. \n",
    "            <form>{result.text}</form>. \n",
    "            Return JSON ONLY, no markdown.\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        fields = json.loads(raw_json.text)[\"fields\"]\n",
    "\n",
    "        for field in fields:\n",
    "            print(field)\n",
    "\n",
    "        return StopEvent(\n",
    "            result=\"Dummy event\"\n",
    "        )  ## Intended to return a StopEvent, but will be updated later\n",
    "\n",
    "    # will be edited in the next section\n",
    "    @step\n",
    "    async def ask_question(self, ctx: Context, ev: QueryEvent) -> StopEvent:\n",
    "        response = self.query_engine.query(\n",
    "            f\"This is a question about the specific resume we have in our database: {ev.query}\"\n",
    "        )\n",
    "        return StopEvent(result=response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9df6593c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./storage/docstore.json.\n",
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./storage/index_store.json.\n",
      "Started parsing the file under job_id 507e6a5a-137f-43f5-81a2-adf89251d9c3\n",
      "Position\n",
      "First Name\n",
      "Last Name\n",
      "Email\n",
      "Phone\n",
      "Linkedin\n",
      "Project Portfolio\n",
      "Degree\n",
      "Graduation Date\n",
      "Current Job Title\n",
      "Current Employer\n",
      "Technical Skills\n",
      "Describe why you’re a good fit for this position\n",
      "Do you have 5 years of experience in React?\n"
     ]
    }
   ],
   "source": [
    "workflow = RAGWorkflow(timeout=60, verbose=False)\n",
    "result = await workflow.run(\n",
    "    resume_file=\"./data/fake_resume.pdf\",\n",
    "    application_form=\"./data/fake_application_form.pdf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2193da",
   "metadata": {},
   "source": [
    "### Generating Questions (second update)\n",
    "\n",
    "Your workflow knows what fields it needs answers for.\n",
    "In this next iteration, you can fire off one `QueryEvent` for each of the fields, so they'll be executed concurrently (we talked about doing concurrent steps in Lesson 2).\n",
    "\n",
    "The changes you're going to make are:\n",
    "* Generate a `QueryEvent` for each of the questions you pulled out of the form\n",
    "* Create a `fill_in_application` step which will take all the responses to the questions and aggregate them into a coherent response\n",
    "* Add a `ResponseEvent` to pass the results of queries to `fill_in_application`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54e4a014",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParseFormEvent(Event):\n",
    "    application_form: str\n",
    "\n",
    "\n",
    "class QueryEvent(Event):\n",
    "    query: str\n",
    "    field: str\n",
    "\n",
    "\n",
    "# new!\n",
    "class ResponseEvent(Event):\n",
    "    field: str\n",
    "    response: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a332c300",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGWorkflow(Workflow):\n",
    "\n",
    "    storage_dir: str = \"./storage\"\n",
    "    llm: OpenAI\n",
    "    query_engine: BaseQueryEngine\n",
    "\n",
    "    @step\n",
    "    async def set_up(self, ctx: Context, ev: StartEvent) -> ParseFormEvent:\n",
    "        if not ev.resume_file:\n",
    "            raise ValueError(\"No resume file provided\")\n",
    "\n",
    "        if not ev.application_form:\n",
    "            raise ValueError(\"No application form provided\")\n",
    "\n",
    "        # define the LLM to work with\n",
    "        self.llm = OpenAI(\n",
    "            model=\"gpt-4.1-nano\",\n",
    "            api_key=openai_api_key,\n",
    "            api_base=os.environ[\"OPENAI_API_BASE\"],\n",
    "            temperature=0.5,\n",
    "        )\n",
    "\n",
    "        # ingest the data and set up the query engine\n",
    "        if os.path.exists(self.storage_dir):\n",
    "            # you've already ingested the resume document\n",
    "            storage_context = StorageContext.from_defaults(persist_dir=self.storage_dir)\n",
    "            index = load_index_from_storage(\n",
    "                storage_context=storage_context,\n",
    "                embed_model=OpenAIEmbedding(\n",
    "                    model_name=\"Cohere-embed-v3-english\",\n",
    "                    api_key=openai_api_key,\n",
    "                    api_base=os.environ[\"OPENAI_API_BASE\"],\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            # parse and load the resume document\n",
    "            documents = LlamaParse(\n",
    "                api_key=llama_cloud_api_key,\n",
    "                # base_url=os.getenv(\"LLAMA_CLOUD_BASE_URL\"),\n",
    "                result_type=ResultType.MD,\n",
    "                user_prompt=\"This is a resume, gather related facts together and format it as bullet points with headers\",\n",
    "            ).load_data(ev.resume_file)\n",
    "\n",
    "            # embed and index the documents\n",
    "            index = VectorStoreIndex.from_documents(\n",
    "                documents=documents,\n",
    "                embed_model=OpenAIEmbedding(\n",
    "                    model_name=\"Cohere-embed-v3-english\",\n",
    "                    api_key=openai_api_key,\n",
    "                    api_base=os.environ[\"OPENAI_API_BASE\"],\n",
    "                ),\n",
    "            )\n",
    "            index.storage_context.persist(persist_dir=self.storage_dir)\n",
    "\n",
    "        # create a query engine\n",
    "        self.query_engine = index.as_query_engine(llm=self.llm, similarity_top_k=5)\n",
    "\n",
    "        # you no longer need a query to be passed in,\n",
    "        # you'll be generating the queries instead\n",
    "        # let's pass the application form to a new step to parse it\n",
    "        return ParseFormEvent(application_form=ev.application_form)\n",
    "\n",
    "    @step\n",
    "    async def parse_form(self, ctx: Context, ev: ParseFormEvent) -> QueryEvent:\n",
    "        parser = LlamaParse(\n",
    "            api_key=llama_cloud_api_key,\n",
    "            # base_url=os.getenv(\"LLAMA_CLOUD_BASE_URL\"),\n",
    "            result_type=ResultType.MD,\n",
    "            user_prompt=\"This is a job application form.\"\n",
    "            \" Create a list of all the fields that need to be filled in.\"\n",
    "            \" Return a bulleted list of the fields ONLY.\",\n",
    "        )\n",
    "\n",
    "        # get the LLM to convert the parsed form into JSON\n",
    "        result = parser.load_data(ev.application_form)[0]\n",
    "        raw_json = self.llm.complete(\n",
    "            f\"\"\"\n",
    "            This is a parsed form. \n",
    "            Convert it into a JSON object containing only the list \n",
    "            of fields to be filled in, in the form {{ fields: [...] }}. \n",
    "            <form>{result.text}</form>. \n",
    "            Return JSON ONLY, no markdown.\n",
    "            \"\"\"\n",
    "        )\n",
    "        fields = json.loads(raw_json.text)[\"fields\"]\n",
    "\n",
    "        # new!\n",
    "        # generate one query for each of the fields, and fire them off\n",
    "        for field in fields:\n",
    "            ctx.send_event(\n",
    "                QueryEvent(\n",
    "                    field=field,\n",
    "                    query=f\"How would you answer this question about the candidate? {field}\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # store the number of fields so we know how many to wait for later\n",
    "        await ctx.store.set(\"total_fields\", len(fields))\n",
    "\n",
    "    @step\n",
    "    async def ask_question(self, ctx: Context, ev: QueryEvent) -> ResponseEvent:\n",
    "        response = self.query_engine.query(\n",
    "            f\"This is a question about the specific resume we have in our database: {ev.query}\"\n",
    "        )\n",
    "        return ResponseEvent(field=ev.field, response=response.response)\n",
    "\n",
    "    # new!\n",
    "    @step\n",
    "    async def fill_in_application(\n",
    "        self, ctx: Context, ev: ResponseEvent\n",
    "    ) -> StopEvent | None:\n",
    "        # get the total number of fields to wait for\n",
    "        total_fields = await ctx.store.get(\"total_fields\")\n",
    "\n",
    "        responses = ctx.collect_events(ev, [ResponseEvent] * total_fields)\n",
    "        if responses is None:\n",
    "            return None  # do nothing if there's nothing to do yet\n",
    "\n",
    "        # we've got all the responses!\n",
    "        responseList = \"\\n\".join(\n",
    "            \"Field: \" + r.field + \"\\n\" + \"Response: \" + r.response for r in responses\n",
    "        )\n",
    "\n",
    "        result = self.llm.complete(\n",
    "            f\"\"\"\n",
    "            You are given a list of fields in an application form and responses to\n",
    "            questions about those fields from a resume. Combine the two into a list of\n",
    "            fields and succinct, factual answers to fill in those fields.\n",
    "\n",
    "            <responses>\n",
    "            {responseList}\n",
    "            </responses>\n",
    "        \"\"\"\n",
    "        )\n",
    "        return StopEvent(result=result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecdd1c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./storage/docstore.json.\n",
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./storage/index_store.json.\n",
      "Started parsing the file under job_id ade3e40c-083b-49e6-ab48-4002476ee3f6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20004/1016046970.py:98: DeprecationWarning: Context.set(key, value) is deprecated. Use 'await ctx.store.set(key, value)' instead.\n",
      "  await ctx.set(\"total_fields\", len(fields))\n",
      "/tmp/ipykernel_20004/1016046970.py:113: DeprecationWarning: Context.get() is deprecated. Use 'await ctx.store.get()' instead.\n",
      "  total_fields = await ctx.get(\"total_fields\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-=-=-=-=- Result -=-=-=-=-\n",
      "- **Position:** Full Stack Developer with extensive experience in developing and maintaining web applications, leading technical teams, and implementing scalable solutions. Expertise in frontend frameworks like React.js, Vue.js, and Next.js; backend technologies including Node.js, Django, and cloud services. Skilled in CI/CD, performance optimization, and accessibility. Proven track record of mentoring junior developers and improving code quality.\n",
      "\n",
      "- **First Name:** Sarah\n",
      "\n",
      "- **Last Name:** Chen\n",
      "\n",
      "- **Email:** sarah.chen@email.com\n",
      "\n",
      "- **Phone:** No phone number provided; contact includes email, LinkedIn, GitHub, and portfolio website.\n",
      "\n",
      "- **LinkedIn:** linkedin.com/in/sarahchen\n",
      "\n",
      "- **Project Portfolio:** Includes EcoTrack, a full-stack carbon footprint tracking app with machine learning; ChatFlow, a real-time, end-to-end encrypted chat app serving over 5,000 monthly users; demonstrates building scalable, user-focused applications with modern technologies; recognized in prominent tech publications.\n",
      "\n",
      "- **Degree:** Bachelor of Science in Computer Science from the University of California, Berkeley\n",
      "\n",
      "- **Graduation Date:** 2017\n",
      "\n",
      "- **Current Job Title:** Senior Full Stack Developer\n",
      "\n",
      "- **Current Employer:** TechFlow Solutions, San Francisco, CA\n",
      "\n",
      "- **Technical Skills:** Frontend: React.js, Redux, Next.js, TypeScript, Vue.js, Nuxt.js, HTML5, CSS3, SASS/SCSS, Jest, React Testing Library, WebPack, Babel; Backend: Node.js, Express.js, Python, Django, GraphQL, REST APIs, PostgreSQL, MongoDB.\n",
      "\n",
      "- **Why I’m a good fit:** Over six years of full-stack development experience with React, Node.js, and cloud architecture. Proven leadership in technical teams, implementing scalable solutions, and performance optimization (e.g., reducing API response times by 60%). Skilled in mentoring junior developers, establishing coding standards, and creating efficient workflows. Experienced in building customer-facing applications, CI/CD pipelines, and ensuring accessibility, committed to delivering high-quality, user-centric solutions.\n",
      "\n",
      "- **React Experience:** Yes, over 5 years of experience with React.js, Redux, Next.js, and TypeScript.\n"
     ]
    }
   ],
   "source": [
    "workflow = RAGWorkflow(timeout=120, verbose=False)\n",
    "\n",
    "result = await workflow.run(\n",
    "    resume_file=\"./data/fake_resume.pdf\",\n",
    "    application_form=\"./data/fake_application_form.pdf\"\n",
    ")\n",
    "\n",
    "print(\"-=-=-=-=- Result -=-=-=-=-\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb269a40",
   "metadata": {},
   "source": [
    "### Workflow Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74488b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./workflows/form_parsing_workflow.html\n"
     ]
    }
   ],
   "source": [
    "WORKFLOW_FILE = \"./workflows/form_parsing_workflow.html\"\n",
    "draw_all_possible_flows(workflow, filename=WORKFLOW_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372c82c8",
   "metadata": {},
   "source": [
    "Your workflow takes all the fields in the form and generates plausible answers for all of them. There are a couple of fields where I think it can do better, and in the next lesson you'll add the ability to give that feedback to the agent and get it to try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e083256",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
