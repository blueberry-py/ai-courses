{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc8016b4",
   "metadata": {},
   "source": [
    "## Lesson 3: Adding RAG\n",
    "\n",
    "Lesson objective: Add a document database to a workflow\n",
    "\n",
    "In this lab, you’ll parse a resume and load it into a vector store, and use the agent to run basic queries against the documents. You’ll use LlamaParse to parse the documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c02fd2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.utils.workflow import draw_all_possible_flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9147d073",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "llama_cloud_api_key = os.environ[\"LLAMA_CLOUD_API_KEY\"]\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "577df799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need nested async for this to work, so let's enable it here. It allows you to nest asyncio event loops within each other.\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d875d5f",
   "metadata": {},
   "source": [
    "### Performing Retrieval-Augmented Generation (RAG) on a Resume Document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1037d53e",
   "metadata": {},
   "source": [
    "#### 1. Parsing the Resume Document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89580fa1",
   "metadata": {},
   "source": [
    "Using LLamaParse, you will transform the resume into a list of Document objects. By default, a Document object stores text along with some other attributes:\n",
    "\n",
    "- metadata: a dictionary of annotations that can be appended to the text.\n",
    "- relationships: a dictionary containing relationships to other Documents.\n",
    "\n",
    "You can tell LlamaParse what kind of document it's parsing, so that it will parse the contents more intelligently. In this case, you tell it that it's reading a resume.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db22d6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_parse import LlamaParse, ResultType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba2d4a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id e0ca9ee3-7ab0-4ec7-a6e2-f580f709b8df\n"
     ]
    }
   ],
   "source": [
    "documents = LlamaParse(\n",
    "    api_key=llama_cloud_api_key,\n",
    "    result_type=ResultType.MD,\n",
    "    user_prompt=\"This is a resume, gather related facts together and format it as bullet points with headers\",\n",
    ").load_data(\n",
    "    \"./data/fake_resume.pdf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c347028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Projects\n",
      "\n",
      "# EcoTrack | GitHub\n",
      "\n",
      "- Built full-stack application for tracking carbon footprint using React, Node.js, and MongoDB\n",
      "- Implemented machine learning algorithm for providing personalized sustainability recommendations\n",
      "- Featured in TechCrunch's \"Top 10 Environmental Impact Apps of 2023\"\n",
      "\n",
      "# ChatFlow | Demo\n",
      "\n",
      "- Developed real-time chat application using WebSocket protocol and React\n",
      "- Implemented end-to-end encryption and message persistence\n",
      "- Serves 5000+ monthly active users\n",
      "\n",
      "# Certifications\n",
      "\n",
      "- AWS Certified Solutions Architect (2023)\n",
      "- Google Cloud Professional Developer (2022)\n",
      "- MongoDB Certified Developer (2021)\n",
      "\n",
      "# Languages\n",
      "\n",
      "- English (Native)\n",
      "- Mandarin Chinese (Fluent)\n",
      "- Spanish (Intermediate)\n",
      "\n",
      "# Interests\n",
      "\n",
      "- Open source contribution\n",
      "- Tech blogging (15K+ Medium followers)\n",
      "- Hackathon mentoring\n",
      "- Rock climbing\n"
     ]
    }
   ],
   "source": [
    "print(documents[2].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff915531",
   "metadata": {},
   "source": [
    "#### 2. Creating a Vector Store Index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a87c4b",
   "metadata": {},
   "source": [
    "The `VectorStoreIndex` will return an index object, which is a data structure that allows you to quickly retrieve relevant context for your query. It's the core foundation for RAG use-cases. You can use indexes to build Query Engines and Chat Engines which enables question & answer and chat over your data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce364dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccafa4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=documents,\n",
    "    embed_model=OpenAIEmbedding(\n",
    "        model_name=\"Cohere-embed-v3-english\",\n",
    "        api_key=openai_api_key,\n",
    "        api_base=os.environ[\"OPENAI_API_BASE\"],\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad4bffb",
   "metadata": {},
   "source": [
    "#### 3. Creating a Query Engine with the Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20ec7a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6310935",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    api_key=openai_api_key,\n",
    "    api_base=os.environ[\"OPENAI_API_BASE\"],\n",
    "    temperature=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cc29382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This person's name is Sarah Chen, and their most recent job was Senior Full Stack Developer at TechFlow Solutions.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(llm=llm, similarity_top_k=5)\n",
    "\n",
    "response = query_engine.query(\n",
    "    \"What is this person's name and what was their most recent job?\"\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8185c4",
   "metadata": {},
   "source": [
    "#### 4. Storing the Index to Disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b0570d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_dir = \"./storage\"\n",
    "\n",
    "index.storage_context.persist(persist_dir=storage_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1d3d88",
   "metadata": {},
   "source": [
    "You can check if your index has already been stored,\n",
    "and if it has, you can reload an index from disk using the load_index_from_storage method, like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f976a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0b667c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./storage/docstore.json.\n",
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./storage/index_store.json.\n"
     ]
    }
   ],
   "source": [
    "# Check if the index is stored on disk\n",
    "if os.path.exists(storage_dir):\n",
    "    # Load the index from disk\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=storage_dir)\n",
    "\n",
    "    restored_index = load_index_from_storage(\n",
    "        storage_context=storage_context,\n",
    "        embed_model=OpenAIEmbedding(\n",
    "            model_name=\"Cohere-embed-v3-english\",\n",
    "            api_key=openai_api_key,\n",
    "            api_base=os.environ[\"OPENAI_API_BASE\"],\n",
    "        ),\n",
    "    )\n",
    "else:\n",
    "    print(\"Index not found on disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b570e650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The person's name is Sarah Chen, and their most recent job was Senior Full Stack Developer at TechFlow Solutions in San Francisco, CA.\n"
     ]
    }
   ],
   "source": [
    "response = restored_index.as_query_engine(llm=llm).query(\n",
    "    \"What is this person's name and what was their most recent job?\"\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e00c88",
   "metadata": {},
   "source": [
    "### Making RAG Agentic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c9833e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import FunctionAgent\n",
    "from llama_index.core.tools import FunctionTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60aa09fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_resume(q: str) -> str:\n",
    "    \"\"\"Answers questions about a specific resume.\"\"\"\n",
    "\n",
    "    # we're using the query engine we already created above\n",
    "    response = query_engine.query(\n",
    "        f\"This is a question about the specific resume we have in our database: {q}\"\n",
    "    )\n",
    "\n",
    "    return response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a1ad900",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_tool = FunctionTool.from_defaults(fn=query_resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f6096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = FunctionAgent(\n",
    "    system_prompt=\"You are an agent capable of using tools to complete tasks\",\n",
    "    tools=[resume_tool],\n",
    "    llm=llm,\n",
    "    timeout=30,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "473a6731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step init_run\n",
      "Step init_run produced event AgentInput\n",
      "Running step setup_agent\n",
      "Step setup_agent produced event AgentSetup\n",
      "Running step run_agent_step\n",
      "Step run_agent_step produced event AgentOutput\n",
      "Running step parse_agent_output\n",
      "Step parse_agent_output produced no event\n",
      "Running step call_tool\n",
      "Step call_tool produced event ToolCallResult\n",
      "Running step aggregate_tool_results\n",
      "Step aggregate_tool_results produced event AgentInput\n",
      "Running step setup_agent\n",
      "Step setup_agent produced event AgentSetup\n",
      "Running step run_agent_step\n",
      "Step run_agent_step produced event AgentOutput\n",
      "Running step parse_agent_output\n",
      "Step parse_agent_output produced event StopEvent\n",
      "The applicant has over 6 years of experience.\n"
     ]
    }
   ],
   "source": [
    "response = await agent.run(\n",
    "    user_msg=\"How many years of experience does the applicant have?\"\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12398c0e",
   "metadata": {},
   "source": [
    "### Wrapping the Agentic RAG into a Workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1557fa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Context,\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ef148d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryEvent(Event):\n",
    "    query: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09872eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.base.base_query_engine import BaseQueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fab6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGWorkflow(Workflow):\n",
    "    storage_dir: str = \"./storage\"\n",
    "    llm: OpenAI\n",
    "    query_engine: BaseQueryEngine\n",
    "\n",
    "    # the first step will be setup\n",
    "    @step\n",
    "    async def set_up(self, ctx: Context, ev: StartEvent) -> QueryEvent:\n",
    "        if not ev.resume_file:\n",
    "            raise ValueError(\"No resume file provided\")\n",
    "\n",
    "        # define an LLM to work with\n",
    "        self.llm = OpenAI(\n",
    "            model=\"gpt-4.1-mini\",\n",
    "            api_key=openai_api_key,\n",
    "            api_base=os.environ[\"OPENAI_API_BASE\"],\n",
    "            temperature=0.5,\n",
    "        )\n",
    "\n",
    "        # ingest the data and set up the query engine\n",
    "        if os.path.exists(self.storage_dir):\n",
    "            # you've already ingested your documents\n",
    "            storage_context = StorageContext.from_defaults(persist_dir=self.storage_dir)\n",
    "\n",
    "            index = load_index_from_storage(\n",
    "                storage_context=storage_context,\n",
    "                embed_model=OpenAIEmbedding(\n",
    "                    model_name=\"Cohere-embed-v3-english\",\n",
    "                    api_key=openai_api_key,\n",
    "                    api_base=os.environ[\"OPENAI_API_BASE\"],\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            # parse and load your documents\n",
    "            documents = LlamaParse(\n",
    "                api_key=llama_cloud_api_key,\n",
    "                result_type=ResultType.MD,\n",
    "                user_prompt=(\n",
    "                    \"This is a resume, gather related facts together and format it as bullet points with headers\"\n",
    "                ),\n",
    "            ).load_data(ev.resume_file)\n",
    "\n",
    "            # embed and index the documents\n",
    "            index = VectorStoreIndex.from_documents(\n",
    "                documents,\n",
    "                embed_model=OpenAIEmbedding(\n",
    "                    model_name=\"Cohere-embed-v3-english\",\n",
    "                    api_key=openai_api_key,\n",
    "                    api_base=os.environ[\"OPENAI_API_BASE\"],\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            index.storage_context.persist(persist_dir=self.storage_dir)\n",
    "\n",
    "        # either way, create a query engine\n",
    "        self.query_engine = index.as_query_engine(llm=self.llm, similarity_top_k=5)\n",
    "\n",
    "        # now fire off a query event to trigger the next step\n",
    "        return QueryEvent(query=ev.query)\n",
    "\n",
    "    # the second step will be to ask a question and return a result immediately\n",
    "    @step\n",
    "    async def ask_question(self, ctx: Context, ev: QueryEvent) -> StopEvent:\n",
    "        response = self.query_engine.query(\n",
    "            f\"This is a question about the specific resume we have in our database: {ev.query}\"\n",
    "        )\n",
    "\n",
    "        return StopEvent(result=response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a6444a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./storage/docstore.json.\n",
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./storage/index_store.json.\n",
      "\n",
      "-=-=-=- Workflow Result -=-=-=-\n",
      "\n",
      "The first place the applicant worked was StartupHub in San Jose, CA as a Junior Web Developer.\n"
     ]
    }
   ],
   "source": [
    "workflow = RAGWorkflow(timeout=120, verbose=False)\n",
    "\n",
    "result = await workflow.run(\n",
    "    resume_file=\"./data/fake_resume.pdf\",\n",
    "    query=\"Where is the first place the applicant worked?\",\n",
    ")\n",
    "\n",
    "print(\"\\n-=-=-=- Workflow Result -=-=-=-\\n\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b3ed64",
   "metadata": {},
   "source": [
    "If you're particularly suspicious, you might notice there's a small bug here: if you run this a second time, with a new resume, this code will find the old resume and not bother to parse it. You don't need to fix that now, but think about how you might fix that.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c68135",
   "metadata": {},
   "source": [
    "### Workflow Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e3baa7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./workflows/rag_workflow.html\n"
     ]
    }
   ],
   "source": [
    "WORKFLOW_FILE = \"./workflows/rag_workflow.html\"\n",
    "draw_all_possible_flows(workflow, filename=WORKFLOW_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d70a09f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
