{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 1: Router Engine\n",
    "https://learn.deeplearning.ai/courses/building-agentic-rag-with-llamaindex/lesson/2/router-query-engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\")\n",
    "\n",
    "EMBEDDING_API_KEY = os.getenv(\"EMBEDDING_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make asyncio loops work inside Jupyter notebooks which already have loops under the hood\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(input_files=[\"./metagpt.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "\n",
    "nodes = splitter.get_nodes_from_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define LLM and Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.jinaai import JinaEmbedding\n",
    "from llama_index.llms.openai_like import OpenAILike\n",
    "\n",
    "Settings.embed_model = JinaEmbedding(api_key=EMBEDDING_API_KEY)\n",
    "Settings.llm = OpenAILike(model=OPENAI_MODEL, temperature=0.01, is_chat_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Summary Index and Vector Index over the Same Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
    "\n",
    "summary_index = SummaryIndex(nodes=nodes)\n",
    "\n",
    "vector_index = VectorStoreIndex(nodes=nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Query Engines and Set Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\", use_async=True\n",
    ")\n",
    "\n",
    "vector_query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=summary_query_engine,\n",
    "    description=\"Useful for summarization questions regarding MetaGPT\",\n",
    ")\n",
    "\n",
    "vector_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=vector_query_engine,\n",
    "    description=\"Useful for retrieving specific context from MetaGPT paper\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Router Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[summary_tool, vector_tool],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 0: The question 'What is the summary of the document?' is general and asks for a summarization, which corresponds to choice 1 that is described as useful for summarization questions..\n",
      "\u001b[0mThe document provides an overview of MetaGPT, a meta-programming framework designed to enhance the capabilities of LLM-based multi-agent systems. It details how MetaGPT incorporates structured communication, role-based collaboration, and efficient information sharing mechanisms to improve code generation and development workflows. The framework's effectiveness is demonstrated through benchmarks and experimental results, showcasing its superiority over previous systems. It also discusses the importance of different roles, an executable feedback mechanism for iterative code improvement, and the overall potential of human-inspired techniques in artificial multi-agent systems. Additionally, the document presents performance metrics, issues, and fixes associated with various versions or iterations of the system.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is the summary of the document?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "print(len(response.source_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 1: The question asks about a specific mechanism or process, which is likely described in the context of the MetaGPT paper. Therefore, choice 2, which is about retrieving specific context from the paper, is more relevant..\n",
      "\u001b[0mAgents share information with other agents by using a shared message pool where they can publish structured messages. They can also subscribe to relevant messages based on their profiles, which allows for efficient role communication within the system.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"How do agents share information with other agents?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 1: The question about ablation study results is specific to the details provided in the MetaGPT paper, which requires retrieving specific context from the paper..\n",
      "\u001b[0mThe ablation study results are not explicitly mentioned in the provided context. However, based on the information given, we can infer that the study involves comparing the performance of MetaGPT without feedback (w/o Feedback) on the SoftwareDev dataset. The results are presented in Table 9, which includes metrics such as code statistics, document statistics, cost statistics, and code executability for 10 randomly selected tasks. The average values of these metrics are also provided, indicating the performance of MetaGPT in terms of code quality, cost, and executability. Unfortunately, specific details about the ablation study, such as the comparison with other variations of MetaGPT or the impact of different components, are not mentioned in the context.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Tell me about the ablation study results?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
